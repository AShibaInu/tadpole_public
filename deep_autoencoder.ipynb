{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"2,3\"\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch.autograd import Variable, Function\n",
    "import time\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import nibabel as nib\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils\n",
    "import torch.optim.lr_scheduler as schd  \n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "torch.backends.cudnn.benchmark=True \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "elim_sparse_feature = True\n",
    "cutoff = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"/home/emy24/tadpole/data/TADPOLE_InputData.csv\", low_memory=False)\n",
    "raw_data = raw_data.rename(columns = {'DX':'DX2'}) # Rename that column so there is no conflict in the algorithm\n",
    "\n",
    "\"\"\"Obtain the IDs used for validation and test set\"\"\"\n",
    "val_id = pd.read_csv(\"/home/emy24/tadpole/data/TADPOLE_TargetData_test.csv\", low_memory=False)\n",
    "test_id = pd.read_csv(\"/home/emy24/tadpole/data/TADPOLE_PredictTargetData_valid.csv\", low_memory=False)\n",
    "\n",
    "val_id = val_id[\"PTID_Key\"].unique().astype(\"int\")\n",
    "test_id = test_id[\"PTID_Key\"].unique().astype(\"int\")\n",
    "\n",
    "\"\"\"Columns 1888,1889, 1890 are numbers. Some number have inequality and some are wrongly typed as string\n",
    " The following lines convert everything to float and replace inqualities with NaN\"\"\"\n",
    "raw_data.iloc[:,1888] = pd.to_numeric(raw_data.iloc[:,1888],errors = \"coerce\")\n",
    "raw_data.iloc[:,1889] = pd.to_numeric(raw_data.iloc[:,1889],errors = \"coerce\")\n",
    "raw_data.iloc[:,1890] = pd.to_numeric(raw_data.iloc[:,1890],errors = \"coerce\")\n",
    "raw_data.iloc[:,3] = raw_data.iloc[:,3].astype('category')\n",
    "raw_data.iloc[:,6] = raw_data.iloc[:,6].astype('category')\n",
    "raw_data.iloc[:,10] = raw_data.iloc[:,10].astype('category')\n",
    "\n",
    "\"\"\" Column 1891 is mixture of string and number. Get rid of the string \"\"\"\n",
    "raw_data.iloc[:,1891] = raw_data.iloc[:,1891].str.extract('(\\d+)', expand = False)\n",
    "raw_data.iloc[:,1891] = pd.to_numeric(raw_data.iloc[:,1891])\n",
    "\n",
    "\"\"\"Remove Columns that does not need one-hot ecoding\"\"\"\n",
    "n_row = raw_data.shape[0]\n",
    "n_col = raw_data.shape[1]\n",
    "removed_col = [] \n",
    "for n in range(n_col):\n",
    "    if (\"PTID_Key\" in raw_data.columns[n] or\n",
    "        \"EXAMDATE\" in raw_data.columns[n] or \n",
    "        \"VERSION\" in raw_data.columns[n] or \n",
    "        \"update\" in raw_data.columns[n] or \n",
    "        \"RUNDATE\" in raw_data.columns[n] or \n",
    "        \"STATUS\" in raw_data.columns[n] or \n",
    "        \"BATCH_UPENNBIOMK9_04_19_17\" in raw_data.columns[n] or \n",
    "        \"KIT_UPENNBIOMK9_04_19_17\" in raw_data.columns[n] or \n",
    "        \"STDS_UPENNBIOMK9_04_19_17\" in raw_data.columns[n]) :\n",
    "        removed_col += [n]    \n",
    "\n",
    "n = np.arange(n_col)\n",
    "n = np.setxor1d(removed_col,n)\n",
    "data = raw_data.iloc[:,n]\n",
    "\n",
    "\"\"\"Search for categorical column and store where NaN are located\"\"\"\n",
    "categorical_col = []\n",
    "for c in range(data.shape[1]):\n",
    "    if ((str(data.iloc[:,c].dtype)) == str(\"category\") or \n",
    "        data.iloc[:,c].dtype is np.dtype('O')):\n",
    "        categorical_col += [data.columns[c]]\n",
    "\n",
    "\"\"\"One-hot encode\"\"\"\n",
    "_nan_categories = data.isnull()\n",
    "data = pd.get_dummies(data)\n",
    "\n",
    "# Put NaN for the categorical data\n",
    "for name in categorical_col:\n",
    "    data.loc[_nan_categories[name], data.columns.str.startswith(name)] = np.NaN\n",
    "\n",
    "\"\"\"Find the Location of NaN. It will be used for the indicator function\"\"\"\n",
    "indicators = pd.isnull(data)\n",
    "# 1 for existing data and 0 to nan\n",
    "indicators = (indicators*1==0)*1\n",
    "\n",
    "\"\"\"Reattach removed columns at the end\"\"\"\n",
    "data = pd.concat([data, raw_data.iloc[:,removed_col]], axis=1)\n",
    "indicators = pd.concat([indicators, raw_data[\"PTID_Key\"]], axis=1) \n",
    "#Only ID is needed for the indicators\n",
    "\n",
    "\"\"\"Replace -4 with NaN\"\"\"\n",
    "data = data.replace(-4, np.NaN)\n",
    "#Make it look pretty :) \n",
    "data = data.replace(np.nan,np.NaN)\n",
    "data = data.replace(np.NAN,np.NaN)\n",
    "\n",
    "\"\"\"Separate Train, Val, Test\"\"\"\n",
    "groups = [data for _, data in data.groupby(\"PTID_Key\")]\n",
    "indicators_group = [indicators for _, indicators in indicators.groupby(\"PTID_Key\")]\n",
    "\n",
    "trn_indicator = []\n",
    "val_indicator = []\n",
    "test_indicator = []\n",
    "xTrain = []\n",
    "xVal = []\n",
    "xTest = []\n",
    "for n in range(len(groups)):\n",
    "    subject = groups[n][\"PTID_Key\"].unique()[0].astype(\"int\")\n",
    "    if np.any(val_id == subject):\n",
    "        xVal += [groups[n]]\n",
    "        val_indicator += [indicators_group[n]]\n",
    "    elif np.any(test_id == subject):\n",
    "        xTest += [groups[n]]\n",
    "        test_indicator += [indicators_group[n]]\n",
    "    else:\n",
    "        xTrain += [groups[n]]\n",
    "        trn_indicator += [indicators_group[n]]\n",
    "    \n",
    "xTrain = pd.concat(xTrain).reset_index(drop=True)\n",
    "xVal = pd.concat(xVal).reset_index(drop=True)\n",
    "xTest = pd.concat(xTest).reset_index(drop=True)\n",
    "\n",
    "trn_indicator = pd.concat(trn_indicator).reset_index(drop=True)\n",
    "val_indicator = pd.concat(val_indicator).reset_index(drop=True)\n",
    "test_indicator = pd.concat(test_indicator).reset_index(drop=True)\n",
    "\n",
    "\"\"\"Standarize the Features. Only xTrain is used to calculate the mean\n",
    "NaN replace by one.\"\"\"\n",
    "trn_mean = []\n",
    "trn_std = []\n",
    "xTrain_norm = xTrain\n",
    "xVal_norm = xVal\n",
    "xTest_norm = xTest\n",
    "for c in range(1833):                   \n",
    "    mean = xTrain.iloc[:,c].mean()\n",
    "    std = xTrain.iloc[:,c].std()\n",
    "    xTrain_norm.iloc[:,c] = (xTrain.iloc[:,c]-mean)/std\n",
    "    xVal_norm.iloc[:,c] = (xVal.iloc[:,c]-mean)/std\n",
    "    xTest_norm.iloc[:,c] = (xTest.iloc[:,c]-mean)/std\n",
    "    trn_mean += [mean]\n",
    "    trn_std += [std]\n",
    "\n",
    "xTrain_norm.iloc[:,:1833] = xTrain_norm.iloc[:,:1833].replace(np.NaN, 0)\n",
    "xVal_norm.iloc[:,:1833] = xVal_norm.iloc[:,:1833].replace(np.NaN, 0)\n",
    "xTest_norm.iloc[:,:1833] = xTest_norm.iloc[:,:1833].replace(np.NaN, 0)\n",
    "\n",
    "# For categorical data, NAN = 0\n",
    "xTrain_norm.iloc[:,1833:] = xTrain_norm.iloc[:,1833:].replace(np.NaN, 0)\n",
    "xVal_norm.iloc[:,1833:] = xVal_norm.iloc[:,1833:].replace(np.NaN, 0)\n",
    "xTest_norm.iloc[:,1833:] = xTest_norm.iloc[:,1833:].replace(np.NaN, 0)\n",
    "\n",
    "\"\"\"Note we need only the first 1943 colums\n",
    "everything else is nonrelevant (dates,update,etc)\"\"\"\n",
    "# The last column has the IDs\n",
    "xTrain_norm = xTrain_norm.iloc[:,:1944]\n",
    "xVal_norm = xVal_norm.iloc[:,:1944]\n",
    "xTest_norm = xTest_norm.iloc[:,:1944]\n",
    "\n",
    "trn_indicator = trn_indicator.iloc[:,:1944]\n",
    "val_indicator = val_indicator.iloc[:,:1944]\n",
    "test_indicator = test_indicator.iloc[:,:1944]\n",
    "\n",
    "\"\"\"Eliminate Feature that few people have \"\"\"\n",
    "if elim_sparse_feature:\n",
    "    percent_filled = np.sum(trn_indicator.as_matrix(), axis = 0)/trn_indicator.shape[0] \n",
    "    #percentage of the column that have data\n",
    "    keep_col = percent_filled > cutoff\n",
    "    keep_col_inx = np.argwhere(keep_col).flatten()\n",
    "\n",
    "    xTrain_norm = xTrain_norm.iloc[:,keep_col_inx]\n",
    "    xVal_norm = xVal_norm.iloc[:,keep_col_inx]\n",
    "    xTest_norm = xTest_norm.iloc[:,keep_col_inx]\n",
    "\n",
    "    trn_indicator = trn_indicator.iloc[:,keep_col_inx]\n",
    "    val_indicator = val_indicator.iloc[:,keep_col_inx]\n",
    "    test_indicator = test_indicator.iloc[:,keep_col_inx]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Autoencoder for Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Convert to numpy array. Note we need only the first 1943 colums\n",
    "everything else is nonrelevant (dates,update,etc)\"\"\"\n",
    "xTrain_norm = xTrain_norm.iloc[:,:1943].as_matrix()\n",
    "xVal_norm = xVal_norm.iloc[:,:1943].as_matrix()\n",
    "xTest_norm = xTest_norm.iloc[:,:1943].as_matrix()\n",
    "\n",
    "trn_indicator = (trn_indicator.iloc[:,:1943].as_matrix()).astype(\"float\")\n",
    "val_indicator = (val_indicator.iloc[:,:1943].as_matrix()).astype(\"float\")\n",
    "test_indicator = (test_indicator.iloc[:,:1943].as_matrix()).astype(\"float\")\n",
    "\n",
    "\"\"\"Convert to Tensor\"\"\"\n",
    "xTrain_norm = torch.from_numpy(xTrain_norm).float()\n",
    "xVal_norm = torch.from_numpy(xVal_norm).float()\n",
    "xTest_norm = torch.from_numpy(xTest_norm).float()\n",
    "\n",
    "trn_indicator = torch.from_numpy(trn_indicator).float()\n",
    "val_indicator = torch.from_numpy(val_indicator).float()\n",
    "test_indicator = torch.from_numpy(test_indicator).float()\n",
    "\n",
    "\"\"\"Dataloader\"\"\"\n",
    "trainset = data_utils.TensorDataset(xTrain_norm, trn_indicator)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size= batch_size, \n",
    "                                          shuffle=True, num_workers= 2, drop_last=False)\n",
    "\n",
    "valset = data_utils.TensorDataset(xVal_norm, val_indicator)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size = batch_size , \n",
    "                                         shuffle=False, num_workers= 2, drop_last=False)\n",
    "\n",
    "testset = data_utils.TensorDataset(xTest_norm, test_indicator)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size = batch_size, \n",
    "                                        shuffle=False, num_workers= 2, drop_last=False)\n",
    "\n",
    "\"\"\"Architecture\"\"\"\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()  \n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(in_features= 1943, out_features= 486),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features= 486, out_features= 243),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features= 243, out_features= 122),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features= 122, out_features= 60),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features= 60, out_features= 122),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features= 122, out_features= 243),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features= 243, out_features= 486),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features= 486, out_features= 1943), \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.dense(x)\n",
    "        return out\n",
    "\n",
    "autoencoder = Autoencoder()\n",
    "autoencoder = torch.nn.DataParallel(autoencoder)\n",
    "autoencoder.cuda() \n",
    "\n",
    "'''Optimazer'''\n",
    "optimizer = optim.Adadelta(autoencoder.parameters())\n",
    "\n",
    "'''Loss Functions'''\n",
    "def adaptive_mse(reconstructed, target, weights):\n",
    "    out = (reconstructed-target)**2\n",
    "    out = out*weights\n",
    "    loss = out.sum()\n",
    "    return loss\n",
    "\n",
    "\"\"\"Training\"\"\"\n",
    "trn_loss = []\n",
    "for epoch in tqdm(range(300)):  \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        autoencoder.train()\n",
    "\n",
    "        inputs, indicator = data\n",
    "        inputs = Variable(inputs).cuda()\n",
    "        indicator = Variable(indicator).cuda()\n",
    "\n",
    "        # Zero the parameter gradients \n",
    "        autoencoder.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        reconstruction = autoencoder(inputs)\n",
    "        loss = adaptive_mse(reconstruction, inputs, indicator)\n",
    "        loss.backward()\n",
    "        optimizer.step()                 \n",
    "\n",
    "        # Calculate statistics\n",
    "        running_loss += loss.data[0]\n",
    "    \n",
    "    trn_loss += [running_loss/(i+1)]\n",
    "    print('[Epoch %d] CAE loss: %.5f' \n",
    "          % (epoch + 1, running_loss/(i+1)))\n",
    "    \n",
    "    \n",
    "#     #Validation\n",
    "#     running_loss = 0.0\n",
    "#     for i, data in enumerate(valloader, 0):       \n",
    "#         autoencoder.eval()\n",
    "\n",
    "#         inputs, labels = data\n",
    "#         labels = labels.view(labels.numel()) #Format the labels the way pytorch wants it \n",
    "#         inputs = Variable(inputs).cuda()\n",
    "#         labels = Variable(labels).cuda()\n",
    "\n",
    "#         reconstruction = cae(inputs)\n",
    "#         yPred = adv(cae(inputs))\n",
    "\n",
    "#         # Calculate statistics\n",
    "#         cae_running_loss += recons_function(reconstruction, inputs).data[0]\n",
    "#         adv_running_loss += discrim_function(yPred, labels).data[0]\n",
    "#         predicted = torch.round(yPred.data)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted.view(-1) == labels.data.view(-1)).sum()\n",
    "\n",
    "#     adv_val_loss = adv_running_loss/(i+1)\n",
    "#     cae_val_loss = cae_running_loss/(i+1)\n",
    "#     adv_val_acc = (100*correct/total)\n",
    "\n",
    "#     print('[Validation] Adv Acc: %.5f  Adv loss: %.5f  CAE loss: %.5f' \n",
    "#           % (adv_val_acc, adv_val_loss, cae_val_loss))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i, data in enumerate(trainloader, 0):       \n",
    "#     autoencoder.eval()\n",
    "\n",
    "#     inputs, indicator = data\n",
    "#     inputs = Variable(inputs).cuda()\n",
    "#     reconstruction = autoencoder(inputs)\n",
    "\n",
    "#     if i == 0:\n",
    "#         target = inputs.data.cpu().numpy()\n",
    "#         reconstructed = reconstruction.data.cpu().numpy()\n",
    "#     else: \n",
    "#         labels = inputs.data.cpu().numpy()\n",
    "#         decoded = reconstruction.data.cpu().numpy()\n",
    "        \n",
    "#         target = np.concatenate((target, labels), axis=0)\n",
    "#         reconstructed = np.concatenate((reconstructed, decoded), axis=0)\n",
    "\n",
    "indicator = pd.DataFrame(indicator.data.cpu().numpy())\n",
    "reconstruction = pd.DataFrame(reconstruction.data.cpu().numpy())\n",
    "inputs = pd.DataFrame(inputs.data.cpu().numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xTrain.iloc[:,:1943]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
